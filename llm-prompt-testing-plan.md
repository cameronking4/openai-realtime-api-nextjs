# LLM-Driven Testing and Prompt Refinement System for Cancer Chat

## Overview

This document outlines a modular, automated system for testing and refining the cancer chat's prompts using Claude LLM. The system will:

1. Create realistic cancer patient personas
2. Simulate conversations between these personas and the cancer chat
3. Evaluate the conversation quality and assessment accuracy
4. Generate prompt improvement suggestions
5. Implement refined prompts in an iterative process

## System Architecture

### 1. Patient Persona Generator Module

```typescript
interface PatientPersona {
  id: string;
  name: string;
  age: number;
  gender: string;
  diagnosis: {
    cancerType: string;
    stage: string;
    timeOfDiagnosis: string;
  };
  treatmentStatus: string;  // 'pre-treatment', 'in-treatment', 'post-treatment', 'palliative'
  psychologicalProfile: {
    anxiety: number;  // 1-10 scale
    depression: number;
    distress: number;
    selfEfficacy: number;
    supportNetworkStrength: number;
  };
  communication: {
    articulationLevel: number;  // 1-10 scale (how well they express themselves)
    openness: number;           // 1-10 scale (willingness to share)
    directness: number;         // 1-10 scale (how straightforward)
    emotionalExpression: number; // 1-10 scale (how emotionally expressive)
  };
  background: {
    familyStatus: string;
    occupation: string;
    importantLifeEvents: string[];
    supportSystem: string[];
  };
  behavioralPatterns: string[];  // E.g., "tends to change subject when discussing prognosis"
  personalConcerns: string[];    // E.g., "worried about impact on children"
}
```

This module will:
- Generate diverse patient personas with varying psychological profiles
- Create a distribution of different cancer types, stages, and treatment statuses
- Include behavioral patterns and communication styles that challenge the cancer chat system
- Save personas to a database for reproducibility and comparison across test runs

### 2. Conversation Simulator Module

```typescript
interface SimulationConfig {
  personaId: string;
  maxTurns: number;
  simulationGoals: string[];  // E.g., "test anxiety assessment accuracy"
  randomness: number;         // Controls how strictly the LLM follows the persona
  recordMetrics: boolean;     // Whether to track metrics during simulation
}

interface SimulationResult {
  conversationId: string;
  transcript: ConversationItem[];  // The full conversation transcript
  assessmentResult: any;           // The assessment generated by the cancer chat
  metrics: {
    conversationLength: number;
    responseDelays: number[];      // Simulated thinking time between responses
    topicChanges: number;
    questionsAsked: number;
    questionsAnswered: number;
    emotionalMoments: Array<{text: string, emotion: string, intensity: number}>;
  };
}
```

This module will:
- Use Claude LLM to simulate a patient based on the persona profile
- Handle the conversation flow with the actual cancer chat system
- Optionally introduce variability in responses to test robustness
- Record the conversation transcript and assessment results
- Track conversation metrics for evaluation

### 3. Assessment Evaluator Module

```typescript
interface EvaluationResult {
  personaId: string;
  conversationId: string;
  promptEffectiveness: {
    naturalness: number;        // 1-10 scale
    efficiency: number;         // 1-10 scale
    userBurden: number;         // 1-10 scale (lower is better)
    coverageOfDomains: number;  // Percentage of assessment domains covered
  };
  assessmentAccuracy: {
    overallAccuracy: number;    // 1-10 scale
    domainSpecificAccuracy: Record<string, number>;  // For each assessment domain
    falsePositives: string[];   // Issues detected that weren't in the persona
    falseNegatives: string[];   // Issues in the persona that weren't detected
  };
  qualitativeAnalysis: string;  // Detailed analysis of the conversation
  promptImprovementSuggestions: string[];  // Specific suggestions for prompt improvement
}
```

This module will:
- Compare the generated assessment against the known persona characteristics
- Evaluate the conversation quality (naturalness, efficiency)
- Identify gaps in the assessment
- Generate specific suggestions for improving both conversation and assessment prompts

### 4. Prompt Refinement Module

```typescript
interface PromptRefinementConfig {
  numberOfIterations: number;
  evaluationsPerIteration: number;
  targetMetrics: {
    minAccuracy: number;
    minNaturalness: number;
    maxUserBurden: number;
  };
  preserveKeyElements: string[];  // Critical elements that must remain in prompts
}

interface RefinementResult {
  initialPrompts: {
    conversation: string;
    assessment: string;
  };
  finalPrompts: {
    conversation: string;
    assessment: string;
  };
  iterationResults: Array<{
    iteration: number;
    evaluationResults: EvaluationResult[];
    promptChanges: {
      conversation: string;
      assessment: string;
    };
    improvementMetrics: {
      accuracyChange: number;
      naturalnessChange: number;
      userBurdenChange: number;
    };
  }>;
  overallImprovement: {
    accuracyChange: number;
    naturalnessChange: number;
    userBurdenChange: number;
  };
}
```

This module will:
- Analyze multiple evaluation results to identify patterns
- Generate refined prompts based on the suggestions
- Maintain a history of prompt versions and their performance
- Implement an iterative refinement process with clear metrics

### 5. Visualization and Monitoring Module

```typescript
interface DashboardData {
  currentIteration: number;
  personasTested: number;
  conversationsSimulated: number;
  metricTrends: {
    accuracy: number[];
    naturalness: number[];
    userBurden: number[];
  };
  promptVersionHistory: Array<{
    version: number;
    prompt: string;
    performance: {
      accuracy: number;
      naturalness: number;
      userBurden: number;
    };
  }>;
  topIssues: string[];
  recentConversations: SimulationResult[];
}
```

This module will:
- Provide a real-time dashboard for monitoring the testing process
- Display key metrics and their trends over iterations
- Allow for inspection of individual conversations
- Highlight specific issues and improvement areas
- Track prompt evolution and effectiveness

## Implementation Approach

Given the existing codebase, here's how I recommend implementing this system:

### Phase 1: Core Components

1. **Setup the testing framework**:
   - Create a new directory in the project: `/test-framework`
   - Implement the base classes for personas, simulations, and evaluations
   - Build integration points with the existing cancer chat system

2. **Implement the Persona Generator**:
   - Create a collection of diverse patient personas
   - Build tooling to randomize and customize persona attributes
   - Store personas in a format that Claude can effectively use for simulation

3. **Build the Conversation Simulator**:
   - Set up API integration with Claude for persona simulation
   - Create a pipeline to connect the Claude simulation with the cancer chat system
   - Implement conversation recording and metric tracking

### Phase 2: Evaluation and Refinement

4. **Develop the Assessment Evaluator**:
   - Implement comparison logic between personas and assessments
   - Create prompt templates for Claude to evaluate conversations
   - Build a scoring system for various aspects of conversation and assessment quality

5. **Create the Prompt Refinement System**:
   - Implement logic to analyze evaluation results and suggest prompt improvements
   - Build a prompt versioning system to track changes
   - Develop mechanisms to preserve critical elements while allowing refinement

### Phase 3: Visualization and Automation

6. **Build the Monitoring Dashboard**:
   - Create a web interface for tracking the testing process
   - Implement visualizations for key metrics
   - Allow for inspection of conversations and assessments

7. **Automate the Testing Process**:
   - Develop scripts to run end-to-end testing cycles
   - Implement scheduled testing with different persona sets
   - Create reporting mechanisms for test results

## Implementation Status

| Module | Status | Progress | Last Updated |
|--------|--------|----------|-------------|
| Project Setup | Completed | 100% | March 12, 2024 |
| Persona Generator | Basic Implementation | 70% | March 12, 2024 |
| Conversation Simulator | Basic Implementation | 70% | March 12, 2024 |
| Assessment Evaluator | Basic Implementation | 70% | March 12, 2024 |
| Prompt Refinement | Not Started | 0% | - |
| Visualization Dashboard | Not Started | 0% | - |
| Test Automation | Not Started | 0% | - |

## Current Implementation Details

The initial implementation provides:

1. **Core Types and Interfaces**:
   - PatientPersona: Detailed model of cancer patient profiles
   - SimulationConfig & SimulationResult: Configuration and results for conversation simulation
   - EvaluationResult: Structure for assessment evaluation results

2. **Persona Generator**:
   - Basic implementation with simulated Claude API responses
   - Support for generating personas with specific characteristics
   - Methods to create diverse sets of personas

3. **Conversation Simulator**:
   - Framework for simulating dialogues between personas and cancer chat
   - Mock implementation of cancer chat client
   - Metrics collection during conversation

4. **Assessment Evaluator**:
   - Tools for comparing assessment results against known persona traits
   - Accuracy scoring across psychological domains
   - Mock implementation of Claude-based evaluation

## Next Steps

1. **Claude API Integration**:
   - Connect to real Claude API instead of using simulated responses
   - Set up proper authentication and rate limiting

2. **Cancer Chat Integration**:
   - Connect to the actual cancer chat system
   - Implement proper transcript handling

3. **Prompt Refinement Module**:
   - Develop prompt version management
   - Implement prompt improvement suggestion aggregation
   - Create prompt testing cycles

4. **Storage and Visualization**:
   - Build result storage mechanism
   - Develop visualization dashboard

## Potential Challenges and Solutions

1. **Challenge**: Ensuring persona diversity and representativeness
   **Solution**: Implement diversity metrics and constraints in the persona generator

2. **Challenge**: Maintaining consistent persona behavior through a conversation
   **Solution**: Use conversation history and periodic persona reminders in Claude prompts

3. **Challenge**: Evaluating subjective aspects of conversation quality
   **Solution**: Use multiple evaluation criteria and human validation for key decisions

4. **Challenge**: Managing computational costs for running Claude
   **Solution**: Implement batching, caching, and selective simulation strategies

5. **Challenge**: Avoiding overfitting prompts to test cases
   **Solution**: Use separate validation sets and periodically introduce new personas

## Next Steps

1. Create base project structure and type definitions
2. Implement the persona generator module
3. Set up the conversation simulator framework
4. Develop a basic assessment evaluation system 